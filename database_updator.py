#! /usr/bin/env python
#  -*- coding: utf-8 -*-
#
# GUI module generated by PAGE version 4.14
# In conjunction with Tcl version 8.6
#	Aug 13, 2018 03:57:27 PM

import urllib.parse
import urllib.request
from urllib.error import HTTPError, URLError
import shutil
import os
import os.path
import sqlite3 as sql
from bs4 import BeautifulSoup
import re



conn = None
cursor = None
alt_cover_list = {
			'7 Killers': 'https://image.ibb.co/fAgv6U/7k.png',
			'Warlock of the Magus World': 'https://image.ibb.co/gEOTt9/600.jpg',
			'Overthrowing Fate': 'https://image.ibb.co/m6SWyK/otf.png',
			'Legends of Ogre Gate': 'https://image.ibb.co/myNLse/loog_1.png',
			'Blue Phoenix': 'https://image.ibb.co/i4q6Xe/bp_1.png',
			'The Divine Elements': 'https://image.ibb.co/ceG58K/tde_1.png',
			'Condemning the Heavens': 'https://image.ibb.co/mTK8TK/cth_1.png'
			}

def download(link, file_name):
	url = urllib.request.Request(
		link,
		data=None,
		headers={
			   'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'
		  }
		)

	with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:
		 shutil.copyfileobj(response, out_file)
		 
def insert_novel(name, url):
	global conn, cursor, alt_cover_list
	filename = "./tmp/novel_"+urllib.parse.quote(name)+".html"
	exception_names_list = {
		'Legend of the Dragon King': 'the-legend-of-the-dragon-king',
		'The Unrivaled Tang Sect': 'douluo-dalu-2-the-unrivaled-tang-sect',
		'Desolate Era': 'the-desolate-era',
		'Stellar Transformations': 'stellar-transformation'
	}
	if name in exception_names_list:
		down = "https://www.novelupdates.com/series/"+exception_names_list[name]+"/"
	else: down = "https://www.novelupdates.com/series/"+name.lower().replace("&", 'and').replace("'", '').replace(" ", '-')+"/"
	try:
		download(down, filename)
	except HTTPError as e:
		# Return code error (e.g. 404, 501, ...)
		if e.code == 404:
			insert_novel2(name, url)
		else:
			print('URL: {}, HTTPError: {} - {}'.format(down, e.code, e.reason))
	except URLError as e:
		# Not an HTTP-specific error (e.g. connection refused)
		print('URL: {}, URLError: {}'.format(down, e.reason))
	else:
		fileHandle = open(filename, "r", encoding = "utf8")
		soup = BeautifulSoup(fileHandle, 'html.parser')
		if name in alt_cover_list:
			img = alt_cover_list[name]
		else: img = soup.find(class_='seriesimg').find('img').get('src')
		autors = ''
		dom_authors = soup.find(id='showauthors').find_all('a')
		for aut in dom_authors:
			if autors != '': autors += ', '
			autors += aut.string
		cursor.execute("INSERT INTO Information(NovelName,link,autor,cover) VALUES(?,?,?,?)", (name, url, autors, img))
		conn.commit()
		fileHandle.close();
		os.remove(filename)
		 
def insert_novel2(name, url):
	global conn, cursor, alt_cover_list
	filename = "./tmp/novel_"+urllib.parse.quote(name)+"_2.html"
	down = url
	try:
		download(down, filename)
	except HTTPError as e:
		print('URL: {}, HTTPError: {} - {}'.format(down, e.code, e.reason))
	except URLError as e:
		print('URL: {}, URLError: {}'.format(down, e.reason))
	else:
		fileHandle = open(filename, "r", encoding = "utf8")
		soup = BeautifulSoup(fileHandle, 'html.parser')
		nov = soup.find(class_='media media-novel-index')
		if name in alt_cover_list:
			img = alt_cover_list[name]
		else: img = nov.find('img').get('src')
		autors = ''
		dom_authors = nov.find_all('p')
		dom_authors += nov.find_all('dl')
		for aut in dom_authors:
			if len(aut.contents) >= 1:
				strmade = ''
				for piece in aut.contents:
					if piece.string is not None:
						strmade += piece.string
				if 'Author:' in strmade or 'Translator:' in strmade:
					clean = strmade.replace('Author:', '').replace('Translator:', '').replace("\n", '').replace("\r", '').replace("\t", '').strip()
					if clean not in autors:
						if autors != '': autors += ', '
						autors += clean
		cursor.execute("INSERT INTO Information(NovelName,link,autor,cover) VALUES(?,?,?,?)", (name, url, autors, img))
		conn.commit()
		fileHandle.close();
		os.remove(filename)
		 
def start():
	global conn, cursor
	conn = sql.connect("novels.db")
	cursor = conn.cursor()
	
	if os.path.isdir('./tmp') is False:
		os.mkdir('./tmp')
	
	filename = "./tmp/bdd_updates.html"
	baseurl = 'https://www.wuxiaworld.com/updates'
	try:
		download(baseurl, filename)
	except HTTPError as e:
		# Return code error (e.g. 404, 501, ...)
		print('URL: {}, HTTPError: {} - {}'.format(baseurl, e.code, e.reason))
	except URLError as e:
		# Not an HTTP-specific error (e.g. connection refused)
		print('URL: {}, URLError: {}'.format(baseurl, e.reason))
	else:
		cursor.execute("DELETE FROM 'Information'")
		conn.commit()
		fileHandle = open(filename, "r", encoding = "utf8")
		soup = BeautifulSoup(fileHandle, 'html.parser')
		tables = soup.find_all(class_="table table-novels")
		for tab in tables:
			novels_dom = tab.find_all(class_="title")
			for title in novels_dom:
				name = title.find('a').string.replace('’', "'").strip()
				url = title.find('a').get('href')
				cursor.execute("SELECT NovelName,link,autor,cover FROM 'Information' WHERE NovelName LIKE ?", (name,))
				row = cursor.fetchone()
				if row is None:
					print('NEW', name)
					insert_novel(name, 'https://www.wuxiaworld.com'+url)
		fileHandle.close()
		os.remove(filename)
		
		filename = "./tmp/bdd_completed.html"
		baseurl = 'https://www.wuxiaworld.com/tag/completed'
		try:
			download(baseurl, filename)
		except HTTPError as e:
			# Return code error (e.g. 404, 501, ...)
			print('URL: {}, HTTPError: {} - {}'.format(baseurl, e.code, e.reason))
		except URLError as e:
			# Not an HTTP-specific error (e.g. connection refused)
			print('URL: {}, URLError: {}'.format(baseurl, e.reason))
		else:
			fileHandle = open(filename, "r", encoding = "utf8")
			soup = BeautifulSoup(fileHandle, 'html.parser')
			tab = soup.find(class_="media-list genres-list")
			novels_dom = tab.find_all(class_="media")
			for title in novels_dom:
				name = title.find('h4').string.replace('’', "'").strip()
				url = title.find('a').get('href')
				cursor.execute("SELECT NovelName,link,autor,cover FROM 'Information' WHERE NovelName LIKE ?", (name,))
				row = cursor.fetchone()
				if row is None:
					print('NEW', name)
					insert_novel(name, 'https://www.wuxiaworld.com'+url)
			fileHandle.close()
			os.remove(filename)
			
		
		print('Database Update Completed')

if __name__ == '__main__':
	start()
